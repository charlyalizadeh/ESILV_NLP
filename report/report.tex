\documentclass[a4paper, 12pt, one column]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage{caption}
\usepackage{float}
\usepackage{float}
\usepackage[top=1.3cm, bottom=2.0cm, outer=2.5cm, inner=2.5cm, heightrounded,
marginparwidth=1.5cm, marginparsep=0.4cm, margin=2.5cm]{geometry}
\usepackage{graphicx} 
\usepackage{hyperref} 
\usepackage{amsmath} 
\usepackage{amsfonts}
\usepackage{amssymb} 
\usepackage{multirow}
\usepackage{layouts}
\usepackage[nameinlink]{cleveref}
%\usepackage{listings}
\usepackage{listingsutf8}
\crefdefaultlabelformat{#2#1#3}
\graphicspath{{images/}}

\renewcommand{\partname}{}
\renewcommand{\thepart}{}

\lstset{basicstyle=\ttfamily, keywordstyle=\bfseries, inputencoding=utf8/latin1}

\begin{document}
\input{titlepage}
\newpage


\section{Data}

\subsection{Cleaning}

The first column to clean was the \lstinline{date} column that we converted into \lstinline{Datetime} object.
All the dates had the same format, for example the first row had the following string for the \lstinline{date} column: \textit{"06 septembre 2021 suite à une expérience en septembre 2021"}.
To convert it into \lstinline[language=python]{Datetime} object we first replace the french months by their respective two digit format (\lstinline{janvier: "01", fevrier: "02", etc}), then we strip the begining trailing whitespaces and only keep the first 10 characters. Then we use the \lstinline{pandas}' function \lstinline{to_datetime()} to convert the column.

We also substracted 1 to all the \lstinline{note} column so that the target starts at 0.



\subsection{Exploration}

First we looked into the stars distribution for all the assureur (\cref{fig:distrib}).


\begin{figure}[H]
    \centering
    \input{images/distrib.pgf}
    \caption{Stars distribution}
    \label{fig:distrib}
\end{figure}

Then we looked into the stars distribution per assureur without and without scaling the y axis (\cref{fig:distrib_split_noscale} and \cref{fig:distrib_split_scale}).

\newgeometry{top=1cm, bottom=0cm}
\begin{figure}[H]
    \advance\leftskip-3cm
    \input{images/distrib_split_noscale.pgf}
    \caption{Stars distribution per assureur}
    \label{fig:distrib_split_noscale}
\end{figure}


\begin{figure}[H]
    \advance\leftskip-3cm
    \input{images/distrib_split_scale.pgf}
    \caption{Stars distribution per assureur (y-axis scaled)}
    \label{fig:distrib_split_scale}
\end{figure}

\restoregeometry

\newgeometry{bottom=0cm}
We also looked at the mean note per assureur, we used a gradient color to show the number of rating per assureur, the gradient intensity is defined by the number of ratings in \cref{fig:mean_note_per_assureur} and by the rank of the assureur (ordered by number of rating) in \cref{fig:mean_note_per_assureur_linear}

\begin{figure}[H]
    \advance\leftskip-3cm
    \input{images/mean_note_per_assureur.pgf}
    \caption{Mean note per assureur (colored by number of ratings)}
    \label{fig:mean_note_per_assureur}
\end{figure}
\begin{figure}[H]
    \advance\leftskip-3cm
    \input{images/mean_note_per_assureur_linear.pgf}
    \caption{Mean note per assureur (colored by rank)}
    \label{fig:mean_note_per_assureur_linear}
\end{figure}
\restoregeometry

And so naturally we looked which assureurs were the most represented in the train dataset (\cref{fig:nbnote_per_assureur}).

\begin{figure}[H]
    \advance\leftskip-2.5cm
    \input{images/nbnote_per_assureur.pgf}
    \caption{Number of note per assureur}
    \label{fig:nbnote_per_assureur}
\end{figure}

Finally we watch the number of review per date using a calendar

\newgeometry{top=1cm, bottom=0cm}
\begin{figure}[H]
    %\advance\leftskip-0cm
    \centering
    \input{images/count_calendar.pgf}
    \caption{Number of review per day}
    \label{fig:count_calendar}
\end{figure}
\restoregeometry

\section{Unsupervised}

We chose to use the Latent Dirichlet Allocation model to extract the main topics of the \lstinline{avis} column.
We trained 6 models in totals, one on all the \lstinline{avis} concatenated, and one for each \lstinline{avis} concatenated by their respective \lstinline{note}.

\subsection{Data processing}

We apply the following processing for the LDA models:
\begin{itemize}
    \item Word-tokenize the \lstinline{avis} using \lstinline{spacy}
    \item Remove punctuation, stop words and words under 3 characters
    \item Strip and lower the words
    \item Lemmatize
    \item Stemmatize
\end{itemize}

This processing applied to the first \lstinline{avis} gives us the following results:

\begin{table}[H]
\centering
\begin{tabular}{|p{7cm}|p{7cm}|}
\hline
\textbf{Original} &
  \textbf{Processed} \\ \hline
Meilleurs assurances, prix, solutions, écoute, rapidité, et je recommande cette compagnie pour vous \textbackslash{}r\textbackslash{}nDes prix attractif et services de qualité et rapidité  &
  meilleur assur prix solut écout rapid recommand compagn prix attract servic qualit rapid \\ \hline
\end{tabular}
\end{table}

\newpage
\subsection{Model}

The \cref{tab:unsupervised} shows the result for the LDA model trained on all the \lstinline{avis} and the \cref{tab:unsupervised_split} shows the results for the LDA models trained on the \lstinline{avis} grouped by their \lstinline{note}.

\begin{table}[H]
\centering
\begin{tabular}{|p{1cm}|p{13cm}|}
\hline
 \textbf{Topic} &                                                                  \textbf{Words} \\
\hline
     0 & [assur, servic, prix, contrat, être, bien, client, demand, fair, mois] \\
\hline
\end{tabular}
\label{tab:unsupervised}
\caption{LDA result}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|p{1cm}|p{13cm}|}
\hline
\textbf{Note} & \textbf{Words} \\
\hline
    0 &    [assur, contrat, mois, aucun, être, demand, fair, rembours, pai, client] \\
    2 & [assur, être, contrat, sinistr, demand, mois, aucun, fair, servic, véhicul] \\
    3 &     [assur, servic, prix, contrat, bien, être, satisf, fair, client, rapid] \\
    4 &       [assur, prix, servic, satisf, rapid, bon, bien, conseil, être, tarif] \\
    5 &  [assur, prix, servic, satisf, rapid, recommand, bon, bien, conseil, tarif] \\
\hline
\end{tabular}
\label{tab:unsupervised_split}
\caption{LDA result on grouped avis}
\end{table}

\newpage
\section{Supervised}

For the supervised task we use word embedding to vectorize the \lstinline{avis} column, then we use this embedding in addition to other features to train multiple models.

\subsection{Data processing}

We use \lstinline{spacy} and \lstinline{fasttext} word embedding to vectorize the \lstinline{avis}, both give 300 size long vectors.
We converted the column \lstinline{assureur} and \lstinline{produit} into categorical variables.
Finally we extracted the \lstinline{dayofweek, day, month} and \lstinline{year} from the \lstinline{date} column, then we droped the \lstinline{auteur} and \lstinline{date} columns.

\subsection{Models}

\subsubsection{Random Forest}


\begin{table}[H]
    \centering
    \begin{tabular}{l|l|l|l|l|l|}
         True\backslash^{\textstyle{\textrm{Predicted}}} & 1 & 2 & 3 & 4 & 5\\ \hline
         1 & 5818 & 0 & 0 & 0 & 0 \\ \hline
         2 & 0 & 2999 & 0 & 0 & 0 \\ \hline
         3 & 0 & 0 & 2745 & 0 & 0 \\ \hline
         4 & 0 & 0 & 0 & 3860 & 0 \\ \hline
         5 & 0 & 0 & 0 & 0 & 3861  \\ \hline
    \end{tabular}
    \caption{Train confusion matrix}
    \label{tab:rfs_train_confusion_matrix}
\end{table}



\begin{table}[H]
    \centering
    \begin{tabular}{l|l|l|l|l|l|}
         True\backslash^{\textstyle{\textrm{Predicted}}} & 1 & 2 & 3 & 4 & 5\\ \hline
         1 & 1319 & 38 & 23 & 47 & 26 \\ \hline
         2 & 592 & 26 & 19 & 52 & 28 \\ \hline
         3 & 285 & 31 & 33 & 178 & 110 \\ \hline
         4 & 168 & 11 & 50 & 412 & 384 \\ \hline
         5 & 99 & 8 & 31 & 318 & 533 \\ \hline
    \end{tabular}
    \caption{Val confusion matrix}
    \label{tab:rfs_val_confusion_matrix}
\end{table}

Validation set accuracy: 0.48185023853972203

\subsubsection{Feed Forward Neural Network}

 
\begin{figure}[H]
    \centering
    \input{images/loss_fr_core_news_md_fr_core_news_lg.pgf}
    \caption{Loss train}
    \label{fig:loss_train}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|}
        \hline
        Accuracy & 0.5394388839910802\\
        \hline
        RMSE & 1.217445418244049 \\
        \hline
    \end{tabular}
    \caption{Train metrics}
    \label{tab:train_metrics}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{l|l|l|l|l|l|}
         True\backslash^{\textstyle{\textrm{Predicted}}} & 1 & 2 & 3 & 4 & 5\\ \hline
        1 & 5592 &   27 &   51 &   79 &   69 \\ \hline
        2 & 2619 &   64 &   92 &  142 &   82 \\ \hline
        3 & 1180 &   90 &  239 &  791 &  445 \\ \hline
        4 & 363 &   54 &  162 & 1956 & 1325 \\ \hline
        5 & 229 &   16 &   63 & 1002 & 2551 \\ \hline
    \end{tabular}
    \caption{Train confusion matrix}
    \label{tab:val_confusion_matrix}
\end{table}


\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|}
        \hline
        Accuracy & 0.5289359054138145 \\
        \hline
        RMSE & 1.3432897739058287 \\
        \hline
    \end{tabular}
    \caption{Val metrics}
    \label{tab:val_metrics}
\end{table}


\begin{table}[H]
    \centering
    \begin{tabular}{l|l|l|l|l|l|}
         True\backslash^{\textstyle{\textrm{Predicted}}} & 1 & 2 & 3 & 4 & 5\\ \hline
         1 & 1384 & 11 & 17 & 24 & 17 \\ \hline
         2 & 606 & 16 & 22 & 50 & 23 \\ \hline
         3 & 276 & 13 & 49 & 188 & 111 \\ \hline
         4 & 121 & 11 & 54 & 472 & 367 \\ \hline
         5 & 75 &  4 & 18 & 263 & 629 \\ \hline
    \end{tabular}
    \caption{Val confusion matrix}
    \label{tab:val_confusion_matrix}
\end{table}


\end{document}
